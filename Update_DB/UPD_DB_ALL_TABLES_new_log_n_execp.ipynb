{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-24 15:52:01,284 - root - INFO - 'main' - Start function\n",
            "2024-05-24 15:52:01,286 - root - INFO - 'create_ssh_tunnel' - Start function\n",
            "2024-05-24 15:52:01,290 - root - INFO - 'create_ssh_tunnel' - Function executed\n",
            "2024-05-24 15:52:01,513 - paramiko.transport - INFO - Connected (version 2.0, client OpenSSH_8.9p1)\n",
            "2024-05-24 15:52:02,364 - paramiko.transport - INFO - Authentication (password) successful!\n",
            "2024-05-24 15:52:02,367 - root - INFO - 'create_db_engine' - Start function\n",
            "2024-05-24 15:52:02,369 - root - INFO - 'create_db_engine' - Function executed\n",
            "2024-05-24 15:52:02,371 - root - INFO - Processing table: sales\n",
            "2024-05-24 15:52:02,372 - root - INFO - 'read_excel_files' - Start function\n",
            "2024-05-24 15:52:02,374 - __main__ - INFO - No Excel files found in the input folder.\n",
            "2024-05-24 15:52:02,375 - root - INFO - 'read_excel_files' - Function executed\n",
            "2024-05-24 15:52:02,376 - root - INFO - 'process_data' - Start function\n",
            "2024-05-24 15:52:02,377 - root - INFO - 'process_data' - Function executed\n",
            "2024-05-24 15:52:02,378 - root - INFO - 'get_intersections' - Start function\n",
            "2024-05-24 15:52:02,378 - root - INFO - 'get_intersections' - Function executed\n",
            "2024-05-24 15:52:02,379 - root - INFO - 'delete_intersections' - Start function\n",
            "2024-05-24 15:52:02,380 - __main__ - INFO - No intersections to delete.\n",
            "2024-05-24 15:52:02,381 - root - INFO - 'delete_intersections' - Function executed\n",
            "2024-05-24 15:52:02,382 - root - INFO - 'load_data_to_db' - Start function\n",
            "2024-05-24 15:52:04,867 - __main__ - ERROR - Unexpected error in wrapper: 'NoneType' object has no attribute 'to_sql'\n",
            "2024-05-24 15:52:04,868 - root - INFO - 'load_excel_sheets' - Start function\n",
            "2024-05-24 15:52:05,650 - root - INFO - 'load_excel_sheets' - Function executed\n",
            "2024-05-24 15:52:05,650 - root - INFO - 'transform_and_load_dict' - Start function\n",
            "2024-05-24 15:52:28,116 - root - INFO - 'transform_and_load_dict' - Function executed\n",
            "2024-05-24 15:52:28,118 - root - INFO - Processing table: ms_sales\n",
            "2024-05-24 15:52:28,118 - root - INFO - 'read_excel_files' - Start function\n",
            "2024-05-24 15:52:28,119 - __main__ - INFO - No Excel files found in the input folder.\n",
            "2024-05-24 15:52:28,120 - root - INFO - 'read_excel_files' - Function executed\n",
            "2024-05-24 15:52:28,120 - root - INFO - 'process_data' - Start function\n",
            "2024-05-24 15:52:28,120 - root - INFO - 'process_data' - Function executed\n",
            "2024-05-24 15:52:28,121 - root - INFO - 'get_intersections' - Start function\n",
            "2024-05-24 15:52:28,121 - root - INFO - 'get_intersections' - Function executed\n",
            "2024-05-24 15:52:28,122 - root - INFO - 'delete_intersections' - Start function\n",
            "2024-05-24 15:52:28,122 - __main__ - INFO - No intersections to delete.\n",
            "2024-05-24 15:52:28,123 - root - INFO - 'delete_intersections' - Function executed\n",
            "2024-05-24 15:52:28,123 - root - INFO - 'load_data_to_db' - Start function\n",
            "2024-05-24 15:52:28,123 - __main__ - ERROR - Unexpected error in wrapper: 'NoneType' object has no attribute 'to_sql'\n",
            "2024-05-24 15:52:28,124 - root - INFO - 'load_excel_sheets' - Start function\n",
            "2024-05-24 15:52:28,873 - root - INFO - 'load_excel_sheets' - Function executed\n",
            "2024-05-24 15:52:28,874 - root - INFO - 'transform_and_load_dict' - Start function\n",
            "2024-05-24 15:52:51,945 - root - INFO - 'transform_and_load_dict' - Function executed\n",
            "2024-05-24 15:52:51,946 - root - INFO - Processing table: ms_stock\n",
            "2024-05-24 15:52:51,946 - root - INFO - 'read_excel_files' - Start function\n",
            "2024-05-24 15:52:51,947 - __main__ - INFO - No Excel files found in the input folder.\n",
            "2024-05-24 15:52:51,948 - root - INFO - 'read_excel_files' - Function executed\n",
            "2024-05-24 15:52:51,948 - root - INFO - 'process_data' - Start function\n",
            "2024-05-24 15:52:51,948 - root - INFO - 'process_data' - Function executed\n",
            "2024-05-24 15:52:51,949 - root - INFO - 'get_intersections' - Start function\n",
            "2024-05-24 15:52:51,950 - root - INFO - 'get_intersections' - Function executed\n",
            "2024-05-24 15:52:51,950 - root - INFO - 'delete_intersections' - Start function\n",
            "2024-05-24 15:52:51,951 - __main__ - INFO - No intersections to delete.\n",
            "2024-05-24 15:52:51,951 - root - INFO - 'delete_intersections' - Function executed\n",
            "2024-05-24 15:52:51,952 - root - INFO - 'load_data_to_db' - Start function\n",
            "2024-05-24 15:52:51,952 - __main__ - ERROR - Unexpected error in wrapper: 'NoneType' object has no attribute 'to_sql'\n",
            "2024-05-24 15:52:51,952 - root - INFO - 'load_excel_sheets' - Start function\n",
            "2024-05-24 15:52:52,790 - root - INFO - 'load_excel_sheets' - Function executed\n",
            "2024-05-24 15:52:52,791 - root - INFO - 'transform_and_load_dict' - Start function\n",
            "2024-05-24 15:53:15,441 - root - INFO - 'transform_and_load_dict' - Function executed\n",
            "2024-05-24 15:53:15,495 - root - INFO - 'main' - Function executed\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import warnings\n",
        "\n",
        "import functools\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, text\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "from sqlalchemy.exc import SQLAlchemyError\n",
        "from sshtunnel import SSHTunnelForwarder, BaseSSHTunnelForwarderError\n",
        "\n",
        "import logging\n",
        "\n",
        "\n",
        "DATA = {\n",
        "    \"sales\": {\n",
        "        \"FOLDER_PATH_IN\": 'C:\\\\Users\\\\dmandree\\\\Downloads\\\\TL_new',\n",
        "        \"FOLDER_PATH_OUT\": 'C:\\\\Users\\\\dmandree\\\\Downloads\\\\TL_arch',\n",
        "        \"SHEET\": 'TurnoverList',\n",
        "        \"COL_NAMES\": ['Day', 'Store', 'Company', 'Open', 'Amount', 'Curr', 'Pcs', 'Rcp', 'People', 'Hours', 'Work', 'Comp:', 'Open_1', 'Amount_1', 'Curr_1', 'Pcs_1', 'Rcp_1', 'People_1', 'Hours_1', 'Work_1'],\n",
        "        \"COMPANIES\": ['Guess Kazakhstan', 'Guess CIS'],\n",
        "        \"SKIP\": 0,\n",
        "        \"IF_EXISTS\": 'append'\n",
        "    },\n",
        "    \"ms_sales\": {\n",
        "        \"FOLDER_PATH_IN\": 'C:\\\\Users\\\\dmandree\\\\Downloads\\\\RTL_new',\n",
        "        \"FOLDER_PATH_OUT\": 'C:\\\\Users\\\\dmandree\\\\Downloads\\\\RTL_arch',\n",
        "        \"SHEET\": 'RTL50000_by_season_by_store old',\n",
        "        \"COL_NAMES\": ['Company', 'Country', 'Day', 'Mfg Season', 'Line Code', 'Gender', 'Dept Group', 'Dept', 'Sub Dept', 'Class', 'Class_1', 'Style', 'Style_1', 'Chain', 'Store', 'Store_1', 'Metrics', 'Ttl Sls Qty', 'TTL Curr Rtl Price €', 'Discount €', 'Ttl Sls €', 'Ttl Cost LC', 'Ttl Sls Trasp Cost LC', 'Ttl Cost €', 'Ttl Sls LC', 'Ttl Sls Trasp Cost €'],\n",
        "        \"COMPANIES\": ['RU', 'KZ'],\n",
        "        \"SKIP\": 3,\n",
        "        \"IF_EXISTS\": 'append'\n",
        "    },\n",
        "    \"ms_stock\": {\n",
        "        \"FOLDER_PATH_IN\": 'C:\\\\Users\\\\dmandree\\\\Downloads\\\\FNC_new',\n",
        "        \"FOLDER_PATH_OUT\": 'C:\\\\Users\\\\dmandree\\\\Downloads\\\\FNC_arch',\n",
        "        \"SHEET\": 'FNC03-50001-Margin_stock all st',\n",
        "        \"COL_NAMES\": ['Company', 'Day', 'Store', 'Store_1', 'Mfg Season', 'Line Code', 'Line_Code_1', 'Style', 'Style_1', 'Sub_Dept', 'Sub_Dept_1', 'Metrics', 'TTL EOH Ttl Qty', 'TTL Loading Cost €', 'TTL Loading Cost LC', 'TTL Trasp Cost €', 'Cost €'],\n",
        "        \"COMPANIES\": ['RU', 'KZ'],\n",
        "        \"SKIP\": 2,\n",
        "        \"IF_EXISTS\": 'replace'\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# Folder for Dict\n",
        "DICT_PATH = 'C:\\\\Users\\\\dmandree\\\\OneDrive - Guess Inc\\\\D Project\\\\Dict\\\\Mapping.xlsx'\n",
        "\n",
        "# List of pages that we transform into dataframes\n",
        "LIST_OF_SHEETS = [\"Stores\", \"Dist_managers\", \"VM\", \"Fin_Calendar_old\", \"Fin_Calendar_new\", \"Template\", \"Start_date\"]\n",
        "\n",
        "# DB and SSH cnnection parameters\n",
        "DB_PARAMS = {\n",
        "    'database': 'postgres',\n",
        "    'user': 'postgres',\n",
        "    'password': '1296',\n",
        "    'host': 'localhost'\n",
        "}\n",
        "\n",
        "SSH_TUNNEL_PARAMS = {\n",
        "    'ssh_address_or_host': ('79.174.86.163', 22),\n",
        "    'ssh_username': 'root',\n",
        "    'ssh_password': 'S0SJcmYwL0ZsmUId',\n",
        "    'remote_bind_address': ('127.0.0.1', 5432),\n",
        "    'local_bind_address': ('127.0.0.1', 8001)\n",
        "}\n",
        "\n",
        "# Logging config\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Adding a formatter to the root logger\n",
        "for handler in logging.root.handlers:\n",
        "    handler.setFormatter(formatter)\n",
        "\n",
        "# Ignore all UserWarnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Function to logging (decorator)\n",
        "def log_function_execution(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        logging.info(f\"'{func.__name__}' - Start function\")\n",
        "        result = func(*args, **kwargs)\n",
        "        logging.info(f\"'{func.__name__}' - Function executed\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "# Function to fix exceptions (decorator)\n",
        "def exception(func):\n",
        "    @functools.wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except FileNotFoundError as e:\n",
        "            logger.error(f\"File not found error in {func.__name__}: {e}\")\n",
        "        except PermissionError as e:\n",
        "            logger.warning(f\"Permission error in {func.__name__}: {e}\")\n",
        "        except (IOError, shutil.Error) as e:\n",
        "            logger.error(f\"Error while moving file {func.__name__}: {e}\")\n",
        "        except ValueError as e:\n",
        "            logger.error(f\"Value error in {func.__name__}: {e}\")\n",
        "        except pd.errors.ParserError as e:\n",
        "            logger.error(f\"Parser error in {func.__name__}: {e}\")\n",
        "        except OSError as e:\n",
        "            logger.error(f\"OS error in {func.__name__}: {e}\")\n",
        "        except BaseSSHTunnelForwarderError as e:\n",
        "            logger.error(f\"SSH tunnel error in {func.__name__}: {e}\")\n",
        "        except SQLAlchemyError as e:\n",
        "            logger.error(f\"SQLAlchemy error in {func.__name__}: {e}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected error in {func.__name__}: {e}\")\n",
        "        return None\n",
        "    return wrapper\n",
        "\n",
        "# Function to read Excel files\n",
        "@exception\n",
        "@log_function_execution\n",
        "def read_excel_files(FOLDER_PATH_IN, FOLDER_PATH_OUT, SHEET, SKIP, COL_NAMES):\n",
        "    if not os.path.exists(FOLDER_PATH_IN):\n",
        "        logger.error(f\"Input folder '{FOLDER_PATH_IN}' does not exist.\")\n",
        "        return None\n",
        "\n",
        "    file_list = os.listdir(FOLDER_PATH_IN)\n",
        "    if not file_list:\n",
        "        logger.info(\"No Excel files found in the input folder.\")\n",
        "        return None\n",
        "    \n",
        "    file_list = os.listdir(FOLDER_PATH_IN)\n",
        "    dfs = []\n",
        "    for file in file_list:\n",
        "        file_path = os.path.join(FOLDER_PATH_IN, file)\n",
        "        with pd.ExcelFile(file_path) as xls:\n",
        "            data = pd.read_excel(xls, sheet_name=SHEET, skiprows=SKIP, names=COL_NAMES)\n",
        "            dfs.append(data)\n",
        "        # Moving the file after processing\n",
        "        move_processed_file(file_path, FOLDER_PATH_OUT, file)\n",
        "    \n",
        "    # Check if the list is not empty    \n",
        "    if dfs:\n",
        "        df = pd.concat(dfs, ignore_index=True)\n",
        "        return df\n",
        "    else:\n",
        "        logger.info(\"No data read from the files.\")\n",
        "        return None\n",
        "    \n",
        "# Function to move file to archive folder\n",
        "@exception\n",
        "@log_function_execution\n",
        "def move_processed_file(file_path, FOLDER_PATH_OUT, file):\n",
        "    if not os.path.exists(FOLDER_PATH_OUT):\n",
        "        os.makedirs(FOLDER_PATH_OUT)\n",
        "\n",
        "    new_path = os.path.join(FOLDER_PATH_OUT, file)\n",
        "    if os.path.exists(new_path):\n",
        "        os.remove(new_path)\n",
        "\n",
        "    shutil.move(file_path, new_path)\n",
        "\n",
        "# Function to create dict data\n",
        "@exception\n",
        "@log_function_execution\n",
        "def load_excel_sheets(DICT_PATH, LIST_OF_SHEETS):\n",
        "    if not os.path.exists(DICT_PATH):\n",
        "        logger.error(f\"The file '{DICT_PATH}' does not exist.\")\n",
        "        return {}\n",
        "\n",
        "    sheets_data = {}\n",
        "    for sheet in LIST_OF_SHEETS:\n",
        "        sheets_data[sheet] = pd.read_excel(DICT_PATH, sheet_name=sheet)\n",
        "    return sheets_data\n",
        "\n",
        "# Function to process data\n",
        "@exception\n",
        "@log_function_execution\n",
        "def process_data(df, COMPANIES):\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    if df['Day'].dtype == 'O':\n",
        "        df['Day'] = df['Day'].str[-10:].str.replace(',', '').str.replace(' ', '')\n",
        "\n",
        "    df[\"Day\"] = pd.to_datetime(df[\"Day\"]).dt.date\n",
        "    df = df.loc[df['Company'].isin(COMPANIES)]\n",
        "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
        "    return df\n",
        "\n",
        "# Function to filtering unique dates in a dataframe\n",
        "@exception\n",
        "@log_function_execution\n",
        "def create_outer_df(df):\n",
        "    unique_combinations = df['day'].unique()\n",
        "    outer_df = pd.DataFrame(unique_combinations, columns=['key'])\n",
        "    return outer_df\n",
        "\n",
        "# Function for creating an SSH tunnel\n",
        "@exception\n",
        "@log_function_execution\n",
        "def create_ssh_tunnel():\n",
        "    ssh_tunnel = SSHTunnelForwarder(**SSH_TUNNEL_PARAMS)\n",
        "    return ssh_tunnel\n",
        "\n",
        "# Function to connecting to a database\n",
        "@exception\n",
        "@log_function_execution\n",
        "def create_db_engine(ssh_tunnel):\n",
        "    if ssh_tunnel is None:\n",
        "        logger.error(\"SSH tunnel is not established.\")\n",
        "        return None\n",
        "\n",
        "    DB_PARAMS['port'] = ssh_tunnel.local_bind_port\n",
        "    engine_str = f\"postgresql://{DB_PARAMS['user']}:{DB_PARAMS['password']}@{DB_PARAMS['host']}:{DB_PARAMS['port']}/{DB_PARAMS['database']}\"\n",
        "    engine = create_engine(engine_str)\n",
        "    return engine\n",
        "\n",
        "# Function to get date intersections\n",
        "@exception\n",
        "@log_function_execution\n",
        "def get_intersections(engine, df):\n",
        "    if df is None or df.empty:\n",
        "        return []\n",
        "\n",
        "    query = text('select DISTINCT day as key from sales')\n",
        "    inner_df = pd.read_sql(query, engine)['key']\n",
        "    inner_df = df['day'].unique()\n",
        "    intersection_df = pd.merge(create_outer_df(df), pd.DataFrame({'key': inner_df}), on='key', how='inner')['key'].tolist()\n",
        "    return intersection_df\n",
        "    \n",
        "# Function to remove intersections from the database\n",
        "@exception\n",
        "@log_function_execution\n",
        "def delete_intersections(session, intersection_df, table_name):\n",
        "    if not intersection_df:\n",
        "        logger.info(\"No intersections to delete.\")\n",
        "        return\n",
        "\n",
        "    delete_query = text(f'DELETE FROM {table_name} WHERE day = ANY(:keys)')\n",
        "    session.execute(delete_query, {'keys': intersection_df})\n",
        "    session.commit()\n",
        "\n",
        "# Function to load data to database\n",
        "@exception\n",
        "@log_function_execution\n",
        "def load_data_to_db(df, engine, name, IF_EXISTS):\n",
        "    with engine.connect() as conn:\n",
        "        df.to_sql(name, conn, if_exists=IF_EXISTS, index=False)\n",
        "\n",
        "# Function to transform and load dict data to database  \n",
        "@exception \n",
        "@log_function_execution\n",
        "def transform_and_load_dict(engine, dfs):\n",
        "    for df_name, df in dfs.items():\n",
        "        df.columns = df.columns.str.lower()\n",
        "        df.to_sql(df_name.lower(), engine, if_exists=\"replace\", index=False)\n",
        "\n",
        "# Main function\n",
        "@log_function_execution\n",
        "def main():\n",
        "        \n",
        "    # Create SSH tunnel\n",
        "    with create_ssh_tunnel() as ssh_tunnel:\n",
        "        \n",
        "        # Create database engine\n",
        "        engine = create_db_engine(ssh_tunnel)\n",
        "        \n",
        "        # Create session\n",
        "        Session = sessionmaker(bind=engine)\n",
        "    \n",
        "        with Session() as session:\n",
        "            # Iterate over dictionary items\n",
        "            for table_name, table_info in DATA.items():  \n",
        "                logging.info(f\"Processing table: {table_name}\")\n",
        "            \n",
        "                # Read Excel files\n",
        "                df = read_excel_files(\n",
        "                                    table_info[\"FOLDER_PATH_IN\"], \n",
        "                                    table_info[\"FOLDER_PATH_OUT\"], \n",
        "                                    table_info[\"SHEET\"], \n",
        "                                    table_info[\"SKIP\"], \n",
        "                                    table_info[\"COL_NAMES\"]\n",
        "                                    )\n",
        "\n",
        "                # Process data\n",
        "                df = process_data(df, table_info[\"COMPANIES\"])\n",
        "                                \n",
        "                # Create intersections\n",
        "                intersection_df = get_intersections(engine, df)\n",
        "                \n",
        "                # Remove intersections from the database\n",
        "                delete_intersections(session, intersection_df, table_name)\n",
        "                \n",
        "                # Load data to database\n",
        "                load_data_to_db(df, engine, table_name, table_info[\"IF_EXISTS\"])   \n",
        "                \n",
        "                # Create Dict \n",
        "                dfs = load_excel_sheets(DICT_PATH, LIST_OF_SHEETS)\n",
        "                \n",
        "                #transform and load dict data to database\n",
        "                transform_and_load_dict(engine, dfs)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
