{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-24 16:19:44,551 - root - INFO - 'main' - Start function\n",
            "2024-05-24 16:19:44,553 - root - INFO - 'create_ssh_tunnel' - Start function\n",
            "2024-05-24 16:19:44,565 - root - INFO - 'create_ssh_tunnel' - Function executed\n",
            "2024-05-24 16:19:44,784 - paramiko.transport - INFO - Connected (version 2.0, client OpenSSH_8.9p1)\n",
            "2024-05-24 16:19:45,452 - paramiko.transport - INFO - Authentication (password) successful!\n",
            "2024-05-24 16:19:45,458 - root - INFO - 'create_db_engine' - Start function\n",
            "2024-05-24 16:19:45,476 - root - INFO - 'create_db_engine' - Function executed\n",
            "2024-05-24 16:19:45,480 - root - INFO - Processing table: sales\n",
            "2024-05-24 16:19:45,481 - root - INFO - 'read_excel_files' - Start function\n",
            "2024-05-24 16:19:45,484 - __main__ - INFO - No Excel files found in the input folder.\n",
            "2024-05-24 16:19:45,485 - root - INFO - 'read_excel_files' - Function executed\n",
            "2024-05-24 16:19:45,488 - root - INFO - 'process_data' - Start function\n",
            "2024-05-24 16:19:45,488 - root - INFO - 'process_data' - Function executed\n",
            "2024-05-24 16:19:45,489 - root - INFO - 'get_intersections' - Start function\n",
            "2024-05-24 16:19:45,490 - root - INFO - 'get_intersections' - Function executed\n",
            "2024-05-24 16:19:45,490 - root - INFO - 'delete_intersections' - Start function\n",
            "2024-05-24 16:19:45,491 - __main__ - INFO - No intersections to delete.\n",
            "2024-05-24 16:19:45,492 - root - INFO - 'delete_intersections' - Function executed\n",
            "2024-05-24 16:19:45,494 - root - INFO - 'load_data_to_db' - Start function\n",
            "2024-05-24 16:19:48,183 - __main__ - ERROR - Unexpected error in wrapper: 'NoneType' object has no attribute 'to_sql'\n",
            "2024-05-24 16:19:48,184 - root - INFO - 'load_excel_sheets' - Start function\n",
            "2024-05-24 16:19:49,164 - root - INFO - 'load_excel_sheets' - Function executed\n",
            "2024-05-24 16:19:49,164 - root - INFO - 'transform_and_load_dict' - Start function\n",
            "2024-05-24 16:20:11,786 - root - INFO - 'transform_and_load_dict' - Function executed\n",
            "2024-05-24 16:20:11,787 - root - INFO - Processing table: ms_sales\n",
            "2024-05-24 16:20:11,787 - root - INFO - 'read_excel_files' - Start function\n",
            "2024-05-24 16:20:11,788 - __main__ - INFO - No Excel files found in the input folder.\n",
            "2024-05-24 16:20:11,789 - root - INFO - 'read_excel_files' - Function executed\n",
            "2024-05-24 16:20:11,789 - root - INFO - 'process_data' - Start function\n",
            "2024-05-24 16:20:11,790 - root - INFO - 'process_data' - Function executed\n",
            "2024-05-24 16:20:11,790 - root - INFO - 'get_intersections' - Start function\n",
            "2024-05-24 16:20:11,791 - root - INFO - 'get_intersections' - Function executed\n",
            "2024-05-24 16:20:11,791 - root - INFO - 'delete_intersections' - Start function\n",
            "2024-05-24 16:20:11,791 - __main__ - INFO - No intersections to delete.\n",
            "2024-05-24 16:20:11,792 - root - INFO - 'delete_intersections' - Function executed\n",
            "2024-05-24 16:20:11,792 - root - INFO - 'load_data_to_db' - Start function\n",
            "2024-05-24 16:20:11,793 - __main__ - ERROR - Unexpected error in wrapper: 'NoneType' object has no attribute 'to_sql'\n",
            "2024-05-24 16:20:11,793 - root - INFO - 'load_excel_sheets' - Start function\n",
            "2024-05-24 16:20:12,563 - root - INFO - 'load_excel_sheets' - Function executed\n",
            "2024-05-24 16:20:12,564 - root - INFO - 'transform_and_load_dict' - Start function\n",
            "2024-05-24 16:20:35,381 - root - INFO - 'transform_and_load_dict' - Function executed\n",
            "2024-05-24 16:20:35,382 - root - INFO - Processing table: ms_stock\n",
            "2024-05-24 16:20:35,383 - root - INFO - 'read_excel_files' - Start function\n",
            "2024-05-24 16:20:35,385 - __main__ - INFO - No Excel files found in the input folder.\n",
            "2024-05-24 16:20:35,386 - root - INFO - 'read_excel_files' - Function executed\n",
            "2024-05-24 16:20:35,387 - root - INFO - 'process_data' - Start function\n",
            "2024-05-24 16:20:35,387 - root - INFO - 'process_data' - Function executed\n",
            "2024-05-24 16:20:35,388 - root - INFO - 'get_intersections' - Start function\n",
            "2024-05-24 16:20:35,390 - root - INFO - 'get_intersections' - Function executed\n",
            "2024-05-24 16:20:35,390 - root - INFO - 'delete_intersections' - Start function\n",
            "2024-05-24 16:20:35,391 - __main__ - INFO - No intersections to delete.\n",
            "2024-05-24 16:20:35,391 - root - INFO - 'delete_intersections' - Function executed\n",
            "2024-05-24 16:20:35,391 - root - INFO - 'load_data_to_db' - Start function\n",
            "2024-05-24 16:20:35,392 - __main__ - ERROR - Unexpected error in wrapper: 'NoneType' object has no attribute 'to_sql'\n",
            "2024-05-24 16:20:35,392 - root - INFO - 'load_excel_sheets' - Start function\n",
            "2024-05-24 16:20:36,217 - root - INFO - 'load_excel_sheets' - Function executed\n",
            "2024-05-24 16:20:36,217 - root - INFO - 'transform_and_load_dict' - Start function\n",
            "2024-05-24 16:20:58,899 - root - INFO - 'transform_and_load_dict' - Function executed\n",
            "2024-05-24 16:20:58,961 - root - INFO - 'main' - Function executed\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import warnings\n",
        "\n",
        "import functools\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import sqlalchemy\n",
        "from sqlalchemy import create_engine, text\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "from sqlalchemy.exc import SQLAlchemyError\n",
        "from sshtunnel import SSHTunnelForwarder, BaseSSHTunnelForwarderError\n",
        "\n",
        "import logging\n",
        "\n",
        "\n",
        "DATA = {\n",
        "    \"sales\": {\n",
        "        \"FOLDER_PATH_IN\": 'C:\\\\Users\\\\dmandree\\\\Downloads\\\\TL_new',\n",
        "        \"FOLDER_PATH_OUT\": 'C:\\\\Users\\\\dmandree\\\\Downloads\\\\TL_arch',\n",
        "        \"SHEET\": 'TurnoverList',\n",
        "        \"COL_NAMES\": ['Day', 'Store', 'Company', 'Open', 'Amount', 'Curr', 'Pcs', 'Rcp', 'People', 'Hours', 'Work', 'Comp:', 'Open_1', 'Amount_1', 'Curr_1', 'Pcs_1', 'Rcp_1', 'People_1', 'Hours_1', 'Work_1'],\n",
        "        \"COMPANIES\": ['Guess Kazakhstan', 'Guess CIS'],\n",
        "        \"SKIP\": 0,\n",
        "        \"IF_EXISTS\": 'append'\n",
        "    },\n",
        "    \"ms_sales\": {\n",
        "        \"FOLDER_PATH_IN\": 'C:\\\\Users\\\\dmandree\\\\Downloads\\\\RTL_new',\n",
        "        \"FOLDER_PATH_OUT\": 'C:\\\\Users\\\\dmandree\\\\Downloads\\\\RTL_arch',\n",
        "        \"SHEET\": 'RTL50000_by_season_by_store old',\n",
        "        \"COL_NAMES\": ['Company', 'Country', 'Day', 'Mfg Season', 'Line Code', 'Gender', 'Dept Group', 'Dept', 'Sub Dept', 'Class', 'Class_1', 'Style', 'Style_1', 'Chain', 'Store', 'Store_1', 'Metrics', 'Ttl Sls Qty', 'TTL Curr Rtl Price €', 'Discount €', 'Ttl Sls €', 'Ttl Cost LC', 'Ttl Sls Trasp Cost LC', 'Ttl Cost €', 'Ttl Sls LC', 'Ttl Sls Trasp Cost €'],\n",
        "        \"COMPANIES\": ['RU', 'KZ'],\n",
        "        \"SKIP\": 3,\n",
        "        \"IF_EXISTS\": 'append'\n",
        "    },\n",
        "    \"ms_stock\": {\n",
        "        \"FOLDER_PATH_IN\": 'C:\\\\Users\\\\dmandree\\\\Downloads\\\\FNC_new',\n",
        "        \"FOLDER_PATH_OUT\": 'C:\\\\Users\\\\dmandree\\\\Downloads\\\\FNC_arch',\n",
        "        \"SHEET\": 'FNC03-50001-Margin_stock all st',\n",
        "        \"COL_NAMES\": ['Company', 'Day', 'Store', 'Store_1', 'Mfg Season', 'Line Code', 'Line_Code_1', 'Style', 'Style_1', 'Sub_Dept', 'Sub_Dept_1', 'Metrics', 'TTL EOH Ttl Qty', 'TTL Loading Cost €', 'TTL Loading Cost LC', 'TTL Trasp Cost €', 'Cost €'],\n",
        "        \"COMPANIES\": ['RU', 'KZ'],\n",
        "        \"SKIP\": 2,\n",
        "        \"IF_EXISTS\": 'replace'\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# Folder for Dict\n",
        "DICT_PATH = 'C:\\\\Users\\\\dmandree\\\\OneDrive - Guess Inc\\\\D Project\\\\Dict\\\\Mapping.xlsx'\n",
        "\n",
        "# List of pages that we transform into dataframes\n",
        "LIST_OF_SHEETS = [\"Stores\", \"Dist_managers\", \"VM\", \"Fin_Calendar_old\", \"Fin_Calendar_new\", \"Template\", \"Start_date\"]\n",
        "\n",
        "# DB and SSH cnnection parameters\n",
        "DB_PARAMS = {\n",
        "    'database': 'postgres',\n",
        "    'user': 'postgres',\n",
        "    'password': '1296',\n",
        "    'host': 'localhost'\n",
        "}\n",
        "\n",
        "SSH_TUNNEL_PARAMS = {\n",
        "    'ssh_address_or_host': ('79.174.86.163', 22),\n",
        "    'ssh_username': 'root',\n",
        "    'ssh_password': 'S0SJcmYwL0ZsmUId',\n",
        "    'remote_bind_address': ('127.0.0.1', 5432),\n",
        "    'local_bind_address': ('127.0.0.1', 8001)\n",
        "}\n",
        "\n",
        "# Logging config\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Adding a formatter to the root logger\n",
        "for handler in logging.root.handlers:\n",
        "    handler.setFormatter(formatter)\n",
        "\n",
        "# Ignore all UserWarnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Decorators\n",
        "# Function to logging (decorator)\n",
        "def log_function_execution(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        logging.info(f\"'{func.__name__}' - Start function\")\n",
        "        result = func(*args, **kwargs)\n",
        "        logging.info(f\"'{func.__name__}' - Function executed\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "# Function to fix exceptions (decorator)\n",
        "def exception(func):\n",
        "    @functools.wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except FileNotFoundError as e:\n",
        "            logger.error(f\"File not found error in {func.__name__}: {e}\")\n",
        "        except PermissionError as e:\n",
        "            logger.warning(f\"Permission error in {func.__name__}: {e}\")\n",
        "        except (IOError, shutil.Error) as e:\n",
        "            logger.error(f\"Error while moving file {func.__name__}: {e}\")\n",
        "        except ValueError as e:\n",
        "            logger.error(f\"Value error in {func.__name__}: {e}\")\n",
        "        except pd.errors.ParserError as e:\n",
        "            logger.error(f\"Parser error in {func.__name__}: {e}\")\n",
        "        except OSError as e:\n",
        "            logger.error(f\"OS error in {func.__name__}: {e}\")\n",
        "        except BaseSSHTunnelForwarderError as e:\n",
        "            logger.error(f\"SSH tunnel error in {func.__name__}: {e}\")\n",
        "        except SQLAlchemyError as e:\n",
        "            logger.error(f\"SQLAlchemy error in {func.__name__}: {e}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected error in {func.__name__}: {e}\")\n",
        "        return None\n",
        "    return wrapper\n",
        "\n",
        "# Function to read Excel files\n",
        "@exception\n",
        "@log_function_execution\n",
        "def read_excel_files(FOLDER_PATH_IN: Path, FOLDER_PATH_OUT: Path, SHEET: str, SKIP: int = 0, COL_NAMES: list[str] | None = None,) -> pd.DataFrame | None:\n",
        "    \"\"\"Reads Excel files from a folder and combines them into a single DataFrame.\n",
        "\n",
        "    Args:\n",
        "        folder_path_in (Path): Path to the folder containing Excel files.\n",
        "        folder_path_out (Path): Path to the folder where processed files are moved.\n",
        "        sheet_name (str): Name of the sheet to read from each Excel file.\n",
        "        skiprows (int, optional): Number of rows to skip at the beginning of each sheet. Defaults to 0.\n",
        "        col_names (list[str], optional): List of column names to use for the resulting DataFrame. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame | None: The combined DataFrame if files were read successfully, otherwise None.\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(FOLDER_PATH_IN):\n",
        "        logger.error(f\"Input folder '{FOLDER_PATH_IN}' does not exist.\")\n",
        "        return None\n",
        "\n",
        "    file_list = os.listdir(FOLDER_PATH_IN)\n",
        "    if not file_list:\n",
        "        logger.info(\"No Excel files found in the input folder.\")\n",
        "        return None\n",
        "    \n",
        "    file_list = os.listdir(FOLDER_PATH_IN)\n",
        "    dfs = []\n",
        "    for file in file_list:\n",
        "        file_path = os.path.join(FOLDER_PATH_IN, file)\n",
        "        with pd.ExcelFile(file_path) as xls:\n",
        "            data = pd.read_excel(xls, sheet_name=SHEET, skiprows=SKIP, names=COL_NAMES)\n",
        "            dfs.append(data)\n",
        "        # Moving the file after processing\n",
        "        move_processed_file(file_path, FOLDER_PATH_OUT, file)\n",
        "    \n",
        "    # Check if the list is not empty    \n",
        "    if dfs:\n",
        "        df = pd.concat(dfs, ignore_index=True)\n",
        "        return df\n",
        "    else:\n",
        "        logger.info(\"No data read from the files.\")\n",
        "        return None\n",
        "    \n",
        "# Function to move file to archive folder\n",
        "@exception\n",
        "@log_function_execution\n",
        "def move_processed_file(file_path: Path, FOLDER_PATH_OUT: Path, file: str) -> None:\n",
        "    \"\"\"Moves a file to the archive folder.\n",
        "\n",
        "    Args:\n",
        "        file_path (Path): Path to the file to move.\n",
        "        folder_path_out (Path): Path to the archive folder.\n",
        "        file_name (str): Name of the file to move.\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(FOLDER_PATH_OUT):\n",
        "        os.makedirs(FOLDER_PATH_OUT)\n",
        "\n",
        "    new_path = os.path.join(FOLDER_PATH_OUT, file)\n",
        "    if os.path.exists(new_path):\n",
        "        os.remove(new_path)\n",
        "\n",
        "    shutil.move(file_path, new_path)\n",
        "\n",
        "# Function to create dict data\n",
        "@exception\n",
        "@log_function_execution\n",
        "def load_excel_sheets(DICT_PATH: Path, LIST_OF_SHEETS: list[str]) -> dict[str, pd.DataFrame]:\n",
        "    \"\"\"Loads data from multiple sheets in an Excel file into a dictionary.\n",
        "\n",
        "    Args:\n",
        "        dict_path (Path): Path to the Excel file.\n",
        "        list_of_sheets (list[str]): List of sheet names to read.\n",
        "\n",
        "    Returns:\n",
        "        dict[str, pd.DataFrame]: Dictionary containing data from each sheet with sheet name as key.\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(DICT_PATH):\n",
        "        logger.error(f\"The file '{DICT_PATH}' does not exist.\")\n",
        "        return {}\n",
        "\n",
        "    sheets_data = {}\n",
        "    for sheet in LIST_OF_SHEETS:\n",
        "        sheets_data[sheet] = pd.read_excel(DICT_PATH, sheet_name=sheet)\n",
        "    return sheets_data\n",
        "\n",
        "# Function to process data\n",
        "@exception\n",
        "@log_function_execution\n",
        "def process_data(df: pd.DataFrame | None, COMPANIES: list[str]) -> pd.DataFrame | None:\n",
        "    \"\"\"Processes a DataFrame by cleaning 'Day' column, filtering companies, and converting columns to lowercase snake_case.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame | None): The DataFrame to process.\n",
        "        companies (list[str]): List of company names to filter the DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame | None: The processed DataFrame, or None if the input DataFrame is empty.\n",
        "    \"\"\"\n",
        "\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    if df['Day'].dtype == 'O':\n",
        "        df['Day'] = df['Day'].str[-10:].str.replace(',', '').str.replace(' ', '')\n",
        "\n",
        "    df[\"Day\"] = pd.to_datetime(df[\"Day\"]).dt.date\n",
        "    df = df.loc[df['Company'].isin(COMPANIES)]\n",
        "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
        "    return df\n",
        "\n",
        "# Function to filtering unique dates in a dataframe\n",
        "@exception\n",
        "@log_function_execution\n",
        "def create_outer_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Creates a DataFrame with a single column containing unique dates from the input DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing the 'day' column.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with a single column named 'key' containing unique dates.\n",
        "    \"\"\"\n",
        "\n",
        "    unique_combinations = df['day'].unique()\n",
        "    outer_df = pd.DataFrame(unique_combinations, columns=['key'])\n",
        "    return outer_df\n",
        "\n",
        "# Function for creating an SSH tunnel\n",
        "@exception\n",
        "@log_function_execution\n",
        "def create_ssh_tunnel() -> SSHTunnelForwarder:\n",
        "    \"\"\"Creates an SSH tunnel using the provided SSH tunnel parameters.\n",
        "\n",
        "    Assumes the existence of an `SSHTunnelForwarder` class and `SSH_TUNNEL_PARAMS` dictionary containing connection details.\n",
        "\n",
        "    Returns:\n",
        "        SSHTunnelForwarder: An instance of the SSH tunnel object.\n",
        "    \"\"\"\n",
        "\n",
        "    ssh_tunnel = SSHTunnelForwarder(**SSH_TUNNEL_PARAMS)\n",
        "    return ssh_tunnel\n",
        "\n",
        "# Function to connecting to a database\n",
        "@exception\n",
        "@log_function_execution\n",
        "def create_db_engine(ssh_tunnel: SSHTunnelForwarder | None) -> sqlalchemy.engine.Engine | None:\n",
        "    \"\"\"Creates a database engine using connection details and an optional SSH tunnel.\n",
        "\n",
        "    Args:\n",
        "        ssh_tunnel (SSHTunnelForwarder | None): An SSH tunnel object for tunneled connection (optional).\n",
        "\n",
        "    Returns:\n",
        "        sqlalchemy.engine.Engine | None: A database engine object, or None if the SSH tunnel is not established.\n",
        "    \"\"\"\n",
        "\n",
        "    if ssh_tunnel is None:\n",
        "        logger.error(\"SSH tunnel is not established.\")\n",
        "        return None\n",
        "\n",
        "    DB_PARAMS['port'] = ssh_tunnel.local_bind_port\n",
        "    engine_str = f\"postgresql://{DB_PARAMS['user']}:{DB_PARAMS['password']}@{DB_PARAMS['host']}:{DB_PARAMS['port']}/{DB_PARAMS['database']}\"\n",
        "    engine = create_engine(engine_str)\n",
        "    return engine\n",
        "\n",
        "# Function to get date intersections\n",
        "@exception\n",
        "@log_function_execution\n",
        "def get_intersections(engine: sqlalchemy.engine.Engine, df: pd.DataFrame | None) -> list[str]:\n",
        "    \"\"\"Retrieves a list of dates intersecting between the DataFrame and the database table.\n",
        "\n",
        "    Args:\n",
        "        engine (sqlalchemy.engine.Engine): The database engine object.\n",
        "        df (pd.DataFrame | None): The DataFrame containing the 'day' column (optional).\n",
        "\n",
        "    Returns:\n",
        "        list[str]: A list of dates present in both the DataFrame and the database table.\n",
        "    \"\"\"\n",
        "\n",
        "    if df is None or df.empty:\n",
        "        return []\n",
        "\n",
        "    query = text('select DISTINCT day as key from sales')\n",
        "    inner_df = pd.read_sql(query, engine)['key']\n",
        "    inner_df = df['day'].unique()\n",
        "    intersection_df = pd.merge(create_outer_df(df), pd.DataFrame({'key': inner_df}), on='key', how='inner')['key'].tolist()\n",
        "    return intersection_df\n",
        "    \n",
        "# Function to remove intersections from the database\n",
        "@exception\n",
        "@log_function_execution\n",
        "def delete_intersections(session: sessionmaker, intersection_df: list[str], table_name: str) -> None:\n",
        "    \"\"\"Deletes data from a database table based on a list of dates.\n",
        "\n",
        "    Args:\n",
        "        session (sessionmaker): A database session object.\n",
        "        intersection_df (list[str]): List of dates to delete.\n",
        "        table_name (str): Name of the database table.\n",
        "    \"\"\"\n",
        "    \n",
        "    if not intersection_df:\n",
        "        logger.info(\"No intersections to delete.\")\n",
        "        return\n",
        "\n",
        "    delete_query = text(f'DELETE FROM {table_name} WHERE day = ANY(:keys)')\n",
        "    session.execute(delete_query, {'keys': intersection_df})\n",
        "    session.commit()\n",
        "\n",
        "# Function to load data to database\n",
        "@exception\n",
        "@log_function_execution\n",
        "def load_data_to_db(df: pd.DataFrame, engine: sqlalchemy.engine.Engine, name: str, IF_EXISTS: str) -> None:\n",
        "    \"\"\"Loads a DataFrame into a database table.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to load.\n",
        "        engine (sqlalchemy.engine.Engine): The database engine object.\n",
        "        name (str): Name of the table to load data into.\n",
        "        IF_EXISTS (str): How to handle existing data in the table ('replace', 'append', or 'fail').\n",
        "    \"\"\"\n",
        "    \n",
        "    with engine.connect() as conn:\n",
        "        df.to_sql(name, conn, if_exists=IF_EXISTS, index=False)\n",
        "\n",
        "# Function to transform and load dict data to database  \n",
        "@exception \n",
        "@log_function_execution\n",
        "def transform_and_load_dict(engine: sqlalchemy.engine.Engine, dfs: dict[str, pd.DataFrame]) -> None:\n",
        "    \"\"\"Transforms and loads data from a dictionary of DataFrames into a database.\n",
        "\n",
        "    Args:\n",
        "        engine (sqlalchemy.engine.Engine): The database engine object.\n",
        "        dfs (dict[str, pd.DataFrame]): A dictionary containing DataFrames with sheet names as keys.\n",
        "    \"\"\"\n",
        "    \n",
        "    for df_name, df in dfs.items():\n",
        "        df.columns = df.columns.str.lower()\n",
        "        df.to_sql(df_name.lower(), engine, if_exists=\"replace\", index=False)\n",
        "\n",
        "# Main function\n",
        "@log_function_execution\n",
        "def main():\n",
        "        \n",
        "    # Create SSH tunnel\n",
        "    with create_ssh_tunnel() as ssh_tunnel:\n",
        "        \n",
        "        # Create database engine\n",
        "        engine = create_db_engine(ssh_tunnel)\n",
        "        \n",
        "        # Create session\n",
        "        Session = sessionmaker(bind=engine)\n",
        "    \n",
        "        with Session() as session:\n",
        "            # Iterate over dictionary items\n",
        "            for table_name, table_info in DATA.items():  \n",
        "                logging.info(f\"Processing table: {table_name}\")\n",
        "            \n",
        "                # Read Excel files\n",
        "                df = read_excel_files(\n",
        "                                    table_info[\"FOLDER_PATH_IN\"], \n",
        "                                    table_info[\"FOLDER_PATH_OUT\"], \n",
        "                                    table_info[\"SHEET\"], \n",
        "                                    table_info[\"SKIP\"], \n",
        "                                    table_info[\"COL_NAMES\"]\n",
        "                                    )\n",
        "\n",
        "                # Process data\n",
        "                df = process_data(df, table_info[\"COMPANIES\"])\n",
        "                                \n",
        "                # Create intersections\n",
        "                intersection_df = get_intersections(engine, df)\n",
        "                \n",
        "                # Remove intersections from the database\n",
        "                delete_intersections(session, intersection_df, table_name)\n",
        "                \n",
        "                # Load data to database\n",
        "                load_data_to_db(df, engine, table_name, table_info[\"IF_EXISTS\"])   \n",
        "                \n",
        "                # Create Dict \n",
        "                dfs = load_excel_sheets(DICT_PATH, LIST_OF_SHEETS)\n",
        "                \n",
        "                #transform and load dict data to database\n",
        "                transform_and_load_dict(engine, dfs)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
